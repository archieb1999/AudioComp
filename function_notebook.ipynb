{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ff6684aa-3987-4f80-8129-9d5fbe86be27",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torchaudio\n",
    "from encodec import EncodecModel\n",
    "from typing import List, Tuple, Optional\n",
    "\n",
    "# Define the type for EncodedFrame\n",
    "EncodedFrame = Tuple[torch.Tensor, Optional[torch.Tensor]]\n",
    "\n",
    "def load_audio(file_path):\n",
    "    \"\"\"Load audio file.\"\"\"\n",
    "    audio, sr = torchaudio.load(file_path)\n",
    "    return audio, sr\n",
    "\n",
    "def get_model_and_sr(channels, bandwidth):\n",
    "    \"\"\"Select the model based on the number of audio channels and bandwidth.\"\"\"\n",
    "    if channels == 1:\n",
    "        if bandwidth not in [1.5, 3, 6, 12, 24]:  # Define acceptable bandwidths for mono\n",
    "            raise ValueError(\"Unsupported bandwidth for mono audio.\")\n",
    "        model = EncodecModel.encodec_model_24khz(pretrained=True)\n",
    "        target_sr = 24000\n",
    "    elif channels == 2:\n",
    "        if bandwidth not in [3, 6, 12, 24]:  # Define acceptable bandwidths for stereo\n",
    "            raise ValueError(\"Unsupported bandwidth for stereo audio.\")\n",
    "        model = EncodecModel.encodec_model_48khz(pretrained=True)\n",
    "        target_sr = 48000\n",
    "    else:\n",
    "        raise ValueError(\"Unsupported number of channels: Encodec supports only mono or stereo.\")\n",
    "    return model, target_sr\n",
    "\n",
    "def resample_audio(audio, original_sr, target_sr):\n",
    "    \"\"\"Resample audio to the target sample rate if necessary.\"\"\"\n",
    "    if original_sr != target_sr:\n",
    "        audio = torchaudio.transforms.Resample(original_sr, target_sr)(audio)\n",
    "    return audio\n",
    "\n",
    "def process_audio(input_file, bandwidth, embedding_file, output_file=None):\n",
    "    \"\"\"\n",
    "    Process an audio file to encode it to embeddings and optionally decode it back.\n",
    "\n",
    "    Args:\n",
    "        input_file (str): Path to the input audio file.\n",
    "        bandwidth (float): Bandwidth to be used for the model.\n",
    "        embedding_file (str): Path where embeddings should be saved.\n",
    "        output_file (str, optional): Path to save the decoded audio. If None, no audio is saved.\n",
    "\n",
    "    \"\"\"\n",
    "    # Load the input audio file\n",
    "    audio, sr = load_audio(input_file)\n",
    "    \n",
    "    # Determine the number of channels in the audio file\n",
    "    channels = audio.shape[0]\n",
    "    \n",
    "    # Get the appropriate model based on the number of channels and specified bandwidth\n",
    "    model, target_sr = get_model_and_sr(channels, bandwidth)\n",
    "    \n",
    "    # Resample the audio to the target sample rate if necessary\n",
    "    audio = resample_audio(audio, sr, target_sr)\n",
    "    \n",
    "    # Configure the model to use the specified bandwidth\n",
    "    model.set_target_bandwidth(bandwidth)\n",
    "    \n",
    "    # Convert the audio to embeddings without modifying the audio data\n",
    "    with torch.no_grad():\n",
    "        audio = audio.unsqueeze(0)  # Add batch dimension for processing\n",
    "        encoded_frames = model.encode(audio)\n",
    "        \n",
    "        # Prepare to save embeddings to a file\n",
    "        embedding_indices = []\n",
    "        for frame, _ in encoded_frames:\n",
    "            embedding_indices.append(frame.cpu().numpy())\n",
    "        \n",
    "        with open(embedding_file, 'w') as f:\n",
    "            f.write(f\"{channels}\\n{bandwidth}\\n\")\n",
    "            for indices in embedding_indices:\n",
    "                for idx in range(indices.shape[1]):\n",
    "                    f.write(' '.join(map(str, indices[0, idx])) + '\\n')\n",
    "    \n",
    "    print(f\"Embeddings saved to: {embedding_file}\")\n",
    "    \n",
    "    # If an output file is provided, decode the embeddings back to audio and save it\n",
    "    if output_file:\n",
    "        output_audio = model.decode(encoded_frames)\n",
    "        torchaudio.save(output_file, output_audio[0], target_sr)\n",
    "        print(f\"Output audio saved to: {output_file}\")\n",
    "\n",
    "\n",
    "def read_embeddings_from_txt(file_path: str, num_codebooks: int) -> List[EncodedFrame]:\n",
    "    \"\"\"\n",
    "    Reads embeddings from a .txt file and converts them to a decodable format.\n",
    "\n",
    "    Args:\n",
    "        file_path (str): Path to the .txt file containing the embeddings.\n",
    "        num_codebooks (int): Number of codebooks used in the model.\n",
    "\n",
    "    Returns:\n",
    "        List[EncodedFrame]: List of encoded frames suitable for decoding.\n",
    "    \"\"\"\n",
    "    encoded_frames: List[EncodedFrame] = []\n",
    "    \n",
    "    with open(file_path, 'r') as file:\n",
    "        lines = file.readlines()\n",
    "\n",
    "    # Initialize a list to store the codebook tensors for each frame\n",
    "    codebook_list = []\n",
    "    \n",
    "    # Skip the first two lines (channels and bandwidth)\n",
    "    for line in lines[2:]:\n",
    "        if line.strip() == '':\n",
    "            continue  # Skip empty lines\n",
    "        \n",
    "        # Split the line into integers\n",
    "        codebook_values = list(map(int, line.strip().split()))\n",
    "        \n",
    "        # Convert the list of integers into a tensor and append to the list\n",
    "        codebook_tensor = torch.tensor(codebook_values, dtype=torch.long)\n",
    "        codebook_list.append(codebook_tensor)\n",
    "        \n",
    "        # Once we have collected all codebooks for a frame, construct the frame tensor\n",
    "        if len(codebook_list) == num_codebooks:\n",
    "            # Stack the codebooks to create the frame tensor of shape [K, T_f]\n",
    "            frame_tensor = torch.stack(codebook_list, dim=0).unsqueeze(0)  # Add batch dimension\n",
    "            encoded_frames.append((frame_tensor, None))  # Append the frame with no scale\n",
    "            codebook_list = []  # Reset the list for the next frame\n",
    "\n",
    "    return encoded_frames\n",
    "\n",
    "def decode_audio_from_embeddings(embedding_file, output_file):\n",
    "    \"\"\"\n",
    "    Decode audio from embeddings saved in a text file.\n",
    "\n",
    "    Args:\n",
    "        embedding_file (str): Path to the .txt file containing the embeddings.\n",
    "        output_file (str): Path to save the decoded audio.\n",
    "    \"\"\"\n",
    "    # Read the channel count and bandwidth from the embedding file\n",
    "    with open(embedding_file, 'r') as f:\n",
    "        lines = f.readlines()\n",
    "        channels = int(lines[0].strip())\n",
    "        bandwidth = float(lines[1].strip())\n",
    "    \n",
    "    # Get the appropriate model based on the number of channels and bandwidth\n",
    "    model, target_sr = get_model_and_sr(channels, bandwidth)\n",
    "    \n",
    "    # Set the target bandwidth for the model\n",
    "    model.set_target_bandwidth(bandwidth)\n",
    "    \n",
    "    # Calculate the number of codebooks based on the model configuration\n",
    "    num_codebooks = int(bandwidth*4/(3*channels)) #1.5 kbps- 2 codebooks, 3 kbps- 4 codebooks,... for mono. For stereo it's half of mono\n",
    "    \n",
    "    # Read the embeddings from the file and convert them into a format suitable for decoding\n",
    "    encoded_frames = read_embeddings_from_txt(embedding_file, num_codebooks)\n",
    "    \n",
    "    # Decode the embeddings into audio\n",
    "    with torch.no_grad():\n",
    "        output_audio = model.decode(encoded_frames)\n",
    "    \n",
    "    # Save the decoded audio to the specified output file\n",
    "    torchaudio.save(output_file, output_audio[0], target_sr)\n",
    "    \n",
    "    print(f\"Decoded audio saved to: {output_file}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "644cbf1e-71e7-4948-b636-38ca59401630",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from encodec import EncodecModel\n",
    "\n",
    "def get_codebooks(audio_type):\n",
    "    \"\"\"\n",
    "    Load the Encodec model based on the audio type and concatenate the codebooks.\n",
    "\n",
    "    Parameters:\n",
    "        audio_type (str): Type of audio model to load. \n",
    "                          Choose 'mono'/'encodec_24khz' for 24kHz model, \n",
    "                          or 'stereo'/'encodec_48khz' for 48kHz model.\n",
    "\n",
    "    Returns:\n",
    "        torch.Tensor: A 3D tensor containing concatenated codebooks with the shape \n",
    "                      [codebook_level, num_vectors_per_codebook, vector_dimension].\n",
    "    \"\"\"\n",
    "    # Map the audio type to the corresponding model\n",
    "    if audio_type in ('mono', 'encodec_24khz'):\n",
    "        model = EncodecModel.encodec_model_24khz(pretrained=True)\n",
    "    elif audio_type in ('stereo', 'encodec_48khz'):\n",
    "        model = EncodecModel.encodec_model_48khz(pretrained=True)\n",
    "    else:\n",
    "        raise ValueError(\"Unsupported audio type. Choose 'mono', 'encodec_24khz', 'stereo', or 'encodec_48khz'.\")\n",
    "\n",
    "    # Access the quantizer\n",
    "    quantizer = model.quantizer\n",
    "\n",
    "    # Collect all codebook tensors from each level of the quantizer\n",
    "    codebook_tensors = [layer._codebook.embed for layer in quantizer.vq.layers]\n",
    "\n",
    "    # Stack all the tensors along a new dimension to create a 3D tensor\n",
    "    # Resulting shape will be [codebook_level, num_vectors_per_codebook, vector_dimension]\n",
    "    concatenated_codebooks = torch.stack(codebook_tensors, dim=0)\n",
    "\n",
    "    return concatenated_codebooks\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
